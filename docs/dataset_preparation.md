# Dataset preparation

[TOC]

### Overview

Our data pipeline converts original dataset to our unified meta-data, with data converters controlled by configs.

### Supported datasets

| Dataset name | Dataset page                                               | Download from public                                         | Download from OpenXRLab                                      |
| ------------ | ---------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Campus       | [Home page](https://campar.in.tum.de/Chair/MultiHumanPose) | [CampusSeq1.tar.bz2](https://www.campar.in.tum.de/public_datasets/2014_cvpr_belagiannis/CampusSeq1.tar.bz2) | [CampusSeq1.tar.bz2](http://10.4.11.59:18080/resources/XRlab/dataset/CampusSeq1.tar.bz2) |
| Shelf        | [Home page](https://campar.in.tum.de/Chair/MultiHumanPose) | [Shelf.tar.bz2](https://www.campar.in.tum.de/public_datasets/2014_cvpr_belagiannis/Shelf.tar.bz2) | [Shelf.tar.bz2](http://10.4.11.59:18080/resources/XRlab/dataset/Shelf.tar.bz2) |
| CMU Panoptic | [Home page](http://domedb.perception.cs.cmu.edu/)          | By [script](https://github.com/CMU-Perceptual-Computing-Lab/panoptic-toolbox/blob/master/scripts/getData.sh) | N/A                                                          |

### Prepare a dataset

Edit config file for data_converter first, set Dataset type and path correctly. If 2D perception data is necessary for your method, set `bbox_detector` and `kps2d_estimator` like `config/data/data_converter/*_w_perception.py`.

```python
type = 'ShelfDataCovnerter'
data_root = 'datasets/Shelf'
bbox_detector = None
kps2d_estimator = None
scene_range = [[300, 600]]
meta_path = 'datasets/Shelf/xrmocap_meta_testset'
visualize = True
```

Run the dataset preparation tool as below, you will find meta-data at `meta_path`.

```bash
python tool/prepare_dataset.py --converter_config config/data/data_converter/campus_data_converter_testset.py
```

### Download converted meta-data

Considering that it takes long to run a converter if perception2d is checked, we have done it for you. Our perception 2D is generated by mmtrack and mmpose, defined in coco_wholebody by default. You can download compressed zip file for converted meta-data below.

| Dataset name | meta name        | Download link                                                | Notes                                                        |
| ------------ | ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Campus       | testset          | [download](http://10.4.11.59:18080/resources/XRlab/dataset/meta-data/CampusSeq1/xrmocap_meta_testset.zip) |                                                              |
| Campus       | testset_mvpose2d | [download](http://10.4.11.59:18080/resources/XRlab/dataset/meta-data/CampusSeq1/xrmocap_meta_testset_mvpose2d.zip) | Perception 2D is generated by [mvpose](https://github.com/zju3dv/mvpose#accelerate-the-evaluation), defined in coco. |
| Campus       | trainset         | [download](http://10.4.11.59:18080/resources/XRlab/dataset/meta-data/CampusSeq1/xrmocap_meta_trainset.zip) |                                                              |
| Shelf        | testset          | [download](http://10.4.11.59:18080/resources/XRlab/dataset/meta-data/Shelf/xrmocap_meta_testset.zip) |                                                              |
| Shelf        | testset_mvpose2d | [download](http://10.4.11.59:18080/resources/XRlab/dataset/meta-data/Shelf/xrmocap_meta_testset_mvpose2d.zip) | Perception 2D is generated by [mvpose](https://github.com/zju3dv/mvpose#accelerate-the-evaluation), defined in coco. There's only data for the first three people in keypoints3d_GT.npz . |
| Shelf        | trainset         | [download](http://10.4.11.59:18080/resources/XRlab/dataset/meta-data/Shelf/xrmocap_meta_trainset.zip) |                                                              |
| CMU Panoptic | 160906_band4     | [download](http://10.4.11.59:18080/resources/XRlab/dataset/meta-data/Panoptic/xrmocap_meta_band4.zip) | Only five views are selected: 03, 06, 12, 13, 23             |
| CMU Panoptic | 160906_ian5      | [download](http://10.4.11.59:18080/resources/XRlab/dataset/meta-data/Panoptic/xrmocap_meta_ian5.zip) | Only five views are selected: 03, 06, 12, 13, 23             |
| CMU Panoptic | 160906_pizza1    | [download](http://10.4.11.59:18080/resources/XRlab/dataset/meta-data/Panoptic/xrmocap_meta_pizza1.zip) | Only five views are selected: 03, 06, 12, 13, 23             |

For CMU panoptic meta-data, frames extracted from videos have been removed before uploading. One has to convert panoptic data locally with `bbox_detector = None` and `kps2d_estimator = None`  first, and then put download data into the converted meta-data directory.

### Visualization

We also offer a tool for visualizing your predicted keypoints3d. To visualize predicted keypoints3d, ground truth keypoints3d, and perception 2d data whose b-box score is higher than 0.96, write a config like this.

```python
type = 'MviewMpersonDataVisualization'
data_root = 'Shelf'
output_dir = 'datasets/Shelf/visualization_output'
meta_path = 'datasets/Shelf/xrmocap_meta_testset'
pred_kps3d_paths = ['datasets/Shelf/xrmocap_meta_testset/predicted_keypoints3d.npz']
bbox_thr = 0.96
vis_percep2d = True
vis_gt_kps3d = True
```

Run the dataset visualization tool as below, you will find videos at `output_dir`.

```bash
python tool/visualize_dataset.py --vis_config config/data/data_visualization/shelf_data_visualization_testset.py
```

For CMU panoptic meta-data, the multi-view all-in-one video is too large to load, please set `vis_aio_video = False`  to avoid OOM.
